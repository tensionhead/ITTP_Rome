{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "% matplotlib inline\n",
    "# The above line is a 'magic' line for the Jupyter notebook "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Towards Data Analysis - Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Python, `pandas` is one of the most commonly-used libraries for data analysis, and has been designed to handle, analyse, and visualise tabular data quickly and easily. If you have data in tabular format, you can avoid the process of building up complex nested data structures: load in your data table, and jump straight to organising, filtering, summarising, etc. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Getting Started \n",
    "\n",
    "First, we need to import `pandas`, which we will do using the standard shortened version of the namespace:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This allows us to access all of the functions and object classes included in the library, via the `pd` namespace. One function included in `pandas`, which we will use now, is `read_table`. We will use this to read our tabular data file into Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_table('speciesDistribution_tabular.txt')\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the output above shows, we now have the full table stored with the variable name `data`. However, to really be able to easily access the data, we need to specify the column headers and row names (header=0, index_col=0). Pandas here already identified the first line as a *header*, however we want to *index* our data set by `taxonID`. This will allow us to access individual rows, columns, or data points *by name*.  We could set the column and row names with the current `data` object, but it's actually easier to simply set them when we first read the table in, so let's edit and re-run our call to `read_table`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_table('speciesDistribution_tabular.txt', index_col = 0)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That looks quite good now! To check what we have interactively, we can access the *index* and *columns* attributes of the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wait, why does that work? Because we are already playing around with the special pandas data type:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Introduction to Data Frames\n",
    "\n",
    "The core data type of pandas is the *DataFrame*, something which rings a bell for **R** users I bet. We will see in the following how much we can use what we already learned about native Python data types (*list* and *dictionary*) and NumPy *arrays* to work with pandas dataframes.\n",
    "\n",
    "We can access individual columns of the dataframe similarly to accessing values in a dictionary, by providing the column title in square brackets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Grimston Wood']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `DOUBLE-TAB` autocompletion to show the values of 'Scoreby Wood'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `.head()` to read top 5 lines of the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `.T` to transpose data (change rows to columns and vice versa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, that we get the row names alongside the count values for our chosen site. If you wanted to, you could access a specific count by also providing the taxon ID:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#test by printing specific values\n",
    "print(data['Hagg Wood']['G'])\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting subsets  - by Name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data.loc[:, :] # entire table, this is same as `data`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When using `.loc`, in the left side of comma a range of rows can be defined/selected and on the right of the comma a range of columns can be indicated. Not that this is the same *row-wise ordering* we've already seen in numpy. For example, the code below selects rows ids from 'H' to 'K' ('H':'K'), and all the columns as no specific range has been defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data.loc['H':'K', :] # table from row id 'H' to 'K'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Exercise 4.1_** \n",
    "\n",
    "Select a subset of the table that contains rows from 'K' until the end, and the columns from 'Hagg Wood' to 'Sutton Wood'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[...] # table from row id 'H' to 'K'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The selected subset can be used for further operations such as getting the 'min' or 'max' values for each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data.loc[...].min()\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting subsets - by Index\n",
    "\n",
    "Sometimes when you have a lot of columns and/or index labels, subsetting by name can be unpractical. Here is where `DataFrame.iloc[...]` comes into play:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first two rows and first three columns\n",
    "data.iloc[:2,:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that accessing values via `.iloc` works **exactly** as for numpy arrays. In fact, under the hood our data is stored in a numpy array.\n",
    "\n",
    "#### Exercise 4.2\n",
    "\n",
    "i) Make a bar chart of species 'B' abundance, label the individual bars according to the habitat name.\n",
    "\n",
    "ii) Now we want to visually compare it to species 'H', extend the bar chart to also show it's abundances. Attach a legend so it's easy to see which species we are plotting. (Hint: taking a look at the [bar chart demo](https://matplotlib.org/gallery/statistics/barchart_demo.html) might help)\n",
    "\n",
    "Bonus: Write a function which takes a list of species names, and plot the abundances again in all habitats. Maybe at some point we are looking for something (say 'P') which is not in our dataframe. Capture such cases and let the user know that the demanded species is not available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary Statistics\n",
    "\n",
    "Pandas builds on top of numpy, therefore many operations working for arrays, also work the same way for dataframes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# total abundance of all species\n",
    "np.sum(data, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total number of organisms per habitat:\n",
    "np.sum(data, axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output looks much nicer (it's still labeled!) compared to working with numpy arrays directly. However, in some cases it's useful to get to it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accessing the underlying np.array\n",
    "a = data.as_matrix()\n",
    "print(a.shape, type(a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note: Even though we called the numpy function `np.sum` on the dataframe, the returned type is a pandas Series, that's why we still got the column/index labels. Series is the smallest unit of pandas data structures, it's like a vector with named coordinates.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 4.3\n",
    "\n",
    "It looks like some species completely disappeared (have value 0) from some habitats. Let's try to find out which species is the most endangered (gone from the most habitats)! (Hint: Boolean indexing also works for dataframes...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# your code here.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Working with DataFrames\n",
    "\n",
    "We don't have time to go into many details about all the many wonderful things that you can do with `pandas` and dataframes. It's almost like a language in itself, and one can easily run a two day (and more..) course just about pandas. In the following we will just discuss a few more basic operations: creation and manipulation of dataframes.\n",
    "\n",
    "#### Dataframe creation\n",
    "\n",
    "So far, by using a `pd.read_...` function pandas created the dataframe for us (how many of these `.read_` functions are there?). This will be enough in many cases, however sometimes one also needs to merge different data sets or you want to store results of an analysis in a new dataframe.\n",
    "\n",
    "There are numerous ways to create dataframes actually, for a start you can put any numpy array into a dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.random import randn\n",
    "import pandas as pd\n",
    "\n",
    "# Gaussian random variables\n",
    "a = randn(10, 5)\n",
    "\n",
    "rand_df = pd.DataFrame(a)\n",
    "rand_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, ok.. but it doesn't look very impressing right? What about some more descriptive labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gaussian random variables\n",
    "a = randn(10, 5)\n",
    "\n",
    "# every 2min\n",
    "sampling_times = np.arange(a.shape[0]) * 2\n",
    "\n",
    "# defining a custom index\n",
    "rand_df = pd.DataFrame(index = sampling_times, data = a)\n",
    "rand_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, at least we have our sampling times now as the index, so if we want to look-up our samples at time point 8min, we can do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_df.loc[8,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which is totally something else than looking at the 9th entry with `.iloc`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_df.iloc[8,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we used just plain numbers as customized index, a sequence of strings is of course also possible (like the taxon ID's in the example).\n",
    "\n",
    "Now wouldn't it be nice to have some labeled columns? We can just create our own column names.\n",
    "\n",
    "#### Exercise 4.4\n",
    "\n",
    "The code below is going in the right direction, but it's full of typos and bugs, can you fix it?!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop over columns of\n",
    "# dataframe\n",
    "cols = {}\n",
    "for i in range( a.shape[0] ):\n",
    "    cols.appent( f'Sample {i}' )\n",
    "    \n",
    "rand_df.columns = cols\n",
    "rand_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In summary, we can create new dataframes by calling `pd.DataFrame(...)`. We can either specify index and column names at creation:\n",
    "```Python\n",
    "my_new_df = pd.DataFrame(index = my_index, \n",
    "            columns = my_column_names,\n",
    "            data = my_data)\n",
    "```\n",
    "or set them to our wishes later:\n",
    "```Python\n",
    "my_new_df = pd.DataFrame( my_data )\n",
    "# make index/column labels \n",
    "# ...\n",
    "my_new_df.index = my_index\n",
    "my_new_df.columns = my_column_names\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Updating DataFrames\n",
    "\n",
    "Our invaluable field workers brought back data for a new habitat surveyed: 'Huntington Wood'. They sended us the new data as a `.csv`, a comma separated, file. Let's load it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new habitat Huntington Wood\n",
    "hw_data = pd.read_csv('HuntingtonWood.csv', index_col = 0)\n",
    "hw_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How can we put that new data into our existing dataframe? Nothing easier than that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Huntington Wood'] = hw_data\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ah, but snap.. instead of denoting absent species with the value 0, now these values are just missing (`NaN`)! Luckily, we know how to deal with missing values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill missing values with 0\n",
    "data = data.fillna(value = 0)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Much better, however we only realize now that the new column is of type `float` instead of `int`. As the experienced numpy cracks we are, we simply cast the whole dataframe into our desired data type:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert everything to integers\n",
    "data.astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aah, now the back-office data wizards can finally get to work to save the planet!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Pandas is a comprehensive library for high-level data analysis.\n",
    "* It has a lot of convenient functionality to read in data, e.g. `pd.read_table()`\n",
    "* It's core data type is the *DataFrame* which can be interfaced both *by names* dictionary style, or *by index* numpy array style\n",
    "* Most functionality of the SciPy stack is directly applicable to dataframes\n",
    "* Missing values can be fixed by `.fillna(fill_value)` \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
